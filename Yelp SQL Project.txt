Data Scientist Role Play: Profiling and Analyzing the Yelp Dataset Coursera Worksheet

This is a 2-part assignment. In the first part, you are asked a series of questions that will help you profile and understand the data just like a data scientist would. For this first part of the assignment, you will be assessed both on the correctness of your findings, as well as the code you used to arrive at your answer. You will be graded on how easy your code is to read, so remember to use proper formatting and comments where necessary.

In the second part of the assignment, you are asked to come up with your own inferences and analysis of the data for a particular research question you want to answer. You will be required to prepare the dataset for the analysis you choose to do. As with the first part, you will be graded, in part, on how easy your code is to read, so use proper formatting and comments to illustrate and communicate your intent as required.

For both parts of this assignment, use this "worksheet." It provides all the questions you are being asked, and your job will be to transfer your answers and SQL coding where indicated into this worksheet so that your peers can review your work. You should be able to use any Text Editor (Windows Notepad, Apple TextEdit, Notepad ++, Sublime Text, etc.) to copy and paste your answers. If you are going to use Word or some other page layout application, just be careful to make sure your answers and code are lined appropriately.
In this case, you may want to save as a PDF to ensure your formatting remains intact for you reviewer.



Part 1: Yelp Dataset Profiling and Understanding

1. Profile the data by finding the total number of records for each of the tables below:
	
i. Attribute table = 10000
ii. Business table = 10000
iii. Category table = 10000
iv. Checkin table = 10000
v. elite_years table = 10000
vi. friend table = 10000
vii. hours table = 10000
viii. photo table = 10000
ix. review table = 10000
x. tip table = 10000
xi. user table = 10000
	
Sample code:

select count(*)
from attribute

Result:
+----------+
| count(*) |
+----------+
|    10000 |
+----------+

2. Find the total distinct records by either the foreign key or primary key for each table. If two foreign keys are listed in the table, please specify which foreign key.

i. Business = 10000
ii. Hours = 1562
iii. Category = 2643
iv. Attribute = 1115
v. Review = 9581 by user_id
vi. Checkin = 493
vii. Photo = 10000
viii. Tip = 537 by user_id
ix. User = 10000
x. Friend = 11
xi. Elite_years = 2780

Sample code:
select count(distinct business_id)
from hours

Result:
+-----------------------------+
| count(distinct business_id) |
+-----------------------------+
|                        1562 |
+-----------------------------+

Note: Primary Keys are denoted in the ER-Diagram with a yellow key icon.	



3. Are there any columns with null values in the Users table? Indicate "yes," or "no."

	Answer: no
	
	
	SQL code used to arrive at answer:
Select
    count(case when id is null then 1 end ) null_id,
    count(case when name is null then 1 end) null_name,
    count(case when review_count is null then 1 end),
    count(case when yelping_since is null then 1 end),
    count(case when useful is null then 1 end),
    count(case when funny is null then 1 end),
    count(case when cool is null then 1 end),
    count(case when fans is null then 1 end),
    count(case when average_stars is null then 1 end),
    count(case when compliment_hot is null then 1 end),
    count(case when compliment_more is null then 1 end),
    count(case when compliment_profile is null then 1 end),
    count(case when compliment_cute is null then 1 end),
    count(case when compliment_list is null then 1 end),
    count(case when compliment_note is null then 1 end),
    count(case when compliment_plain is null then 1 end),
    count(case when compliment_funny is null then 1 end),
    count(case when compliment_writer is null then 1 end),
    count(case when compliment_photos is null then 1 end)

from user
	
	

	
4. For each table and column listed below, display the smallest (minimum), largest (maximum), and average (mean) value for the following fields:

	i. Table: Review, Column: Stars
	
		min:	 1	max: 5		avg: 3.7082
		
	
	ii. Table: Business, Column: Stars
	
		min:	1	max:	5	avg: 3.6549
		
	
	iii. Table: Tip, Column: Likes
	
		min:	0	max:	2	avg: 0.0144
		
	
	iv. Table: Checkin, Column: Count
	
		min:	1	max:	53	avg: 1.9414
		
	
	v. Table: User, Column: Review_count
	
		min:	0	max:	2000	avg: 24.2995

Sample code:
select
    min(stars) min_stars,
    max(stars) max_stars,
    avg(stars) avg_stars
from review

Result:
+-----------+-----------+-----------+
| min_stars | max_stars | avg_stars |
+-----------+-----------+-----------+
|         1 |         5 |    3.7082 |
+-----------+-----------+-----------+
		


5. List the cities with the most reviews in descending order:

	SQL code used to arrive at answer:
	
select city, sum(review_count)
from business
group by city
order by sum(review_count) desc

	Copy and Paste the Result Below:
+-----------------+-------------------+
| city            | sum(review_count) |
+-----------------+-------------------+
| Las Vegas       |             82854 |
| Phoenix         |             34503 |
| Toronto         |             24113 |
| Scottsdale      |             20614 |
| Charlotte       |             12523 |
| Henderson       |             10871 |
| Tempe           |             10504 |
| Pittsburgh      |              9798 |
| Montréal        |              9448 |
| Chandler        |              8112 |
| Mesa            |              6875 |
| Gilbert         |              6380 |
| Cleveland       |              5593 |
| Madison         |              5265 |
| Glendale        |              4406 |
| Mississauga     |              3814 |
| Edinburgh       |              2792 |
| Peoria          |              2624 |
| North Las Vegas |              2438 |
| Markham         |              2352 |
| Champaign       |              2029 |
| Stuttgart       |              1849 |
| Surprise        |              1520 |
| Lakewood        |              1465 |
| Goodyear        |              1155 |
+-----------------+-------------------+
(Output limit exceeded, 25 of 362 total rows shown)
	
	
6. Find the distribution of star ratings to the business in the following cities:

i. Avon

SQL code used to arrive at answer:
select
    b.stars stars_rating, count(b.id) count
from business b
where b.city = 'Avon'
group by b.stars

Copy and Paste the Resulting Table Below (2 columns – star rating and count):

+--------------+-------+
| stars_rating | count |
+--------------+-------+
|          1.5 |     1 |
|          2.5 |     2 |
|          3.5 |     3 |
|          4.0 |     2 |
|          4.5 |     1 |
|          5.0 |     1 |
+--------------+-------+

ii. Beachwood

SQL code used to arrive at answer:
select
    b.stars stars_rating, count(b.id) count
from business b
where b.city = 'Beachwood'
group by b.stars

Copy and Paste the Resulting Table Below (2 columns – star rating and count):
+--------------+-------+
| stars_rating | count |
+--------------+-------+
|          2.0 |     1 |
|          2.5 |     1 |
|          3.0 |     2 |
|          3.5 |     2 |
|          4.0 |     1 |
|          4.5 |     2 |
|          5.0 |     5 |
+--------------+-------+
	


7. Find the top 3 users based on their total number of reviews:
		
	SQL code used to arrive at answer:
	select
    		u.name, u.id, count(r.id)
	from review r
	join user u
	on r.user_id = u.id
	group by u.name
	order by count(r.id) desc
	limit 3
		
	Copy and Paste the Result Below:
+-----------+------------------------+-------------+
| name      | id                     | count(r.id) |
+-----------+------------------------+-------------+
| Ed        | -fUARDNuXAfrOn4WLSZLgA |           3 |
| Amy       | -lax4vB95WPfnLJcUrVd_w |           2 |
| Christina | -Biq3Dt8YhkRJEO_ITrvww |           2 |
+-----------+------------------------+-------------+


8. Does posing more reviews correlate with more fans?

	Please explain your findings and interpretation of the results:
	From the table below, posing more reviews does not correlate with more fans.

Sample code:
	select
    		name, review_count,fans
	from user
    	order by review_count desc
Result:
+-----------+--------------+------+
| name      | review_count | fans |
+-----------+--------------+------+
| Gerald    |         2000 |  253 |
| Sara      |         1629 |   50 |
| Yuri      |         1339 |   76 |
| .Hon      |         1246 |  101 |
| William   |         1215 |  126 |
| Harald    |         1153 |  311 |
| eric      |         1116 |   16 |
| Roanna    |         1039 |  104 |
| Mimi      |          968 |  497 |
| Christine |          930 |  173 |
| Ed        |          904 |   38 |
| Nicole    |          864 |   43 |
| Fran      |          862 |  124 |
| Mark      |          861 |  115 |
| Christina |          842 |   85 |
| Dominic   |          836 |   37 |
| Lissa     |          834 |  120 |
| Lisa      |          813 |  159 |
| Alison    |          775 |   61 |
| Sui       |          754 |   78 |
| Tim       |          702 |   35 |
| L         |          696 |   10 |
| Angela    |          694 |  101 |
| Crissy    |          676 |   25 |
| Lyn       |          675 |   45 |
+-----------+--------------+------+
(Output limit exceeded, 25 of 10000 total rows shown)
	
9. Are there more reviews with the word "love" or with the word "hate" in them?

	Answer: More reviews with the word love

	
	SQL code used to arrive at answer:

	select
            count(case when lower(text) like '%love%' then 1 end) love_count,
            count(case when lower(text) like '%hate%' then 1 end) hate_count
	from review
+------------+------------+
| love_count | hate_count |
+------------+------------+
|       1780 |        232 |
+------------+------------+
	
10. Find the top 10 users with the most fans:

	SQL code used to arrive at answer:
	select
        	id, name, fans
	from user
    	order by fans desc
    	limit 10
	
	Copy and Paste the Result Below:

+------------------------+-----------+------+
| id                     | name      | fans |
+------------------------+-----------+------+
| -9I98YbNQnLdAmcYfb324Q | Amy       |  503 |
| -8EnCioUmDygAbsYZmTeRQ | Mimi      |  497 |
| --2vR0DIsmQ6WfcSzKWigw | Harald    |  311 |
| -G7Zkl1wIWBBmD0KRy_sCw | Gerald    |  253 |
| -0IiMAZI2SsQ7VmyzJjokQ | Christine |  173 |
| -g3XIcCb2b-BD0QBCcq2Sw | Lisa      |  159 |
| -9bbDysuiWeo2VShFJJtcw | Cat       |  133 |
| -FZBTkAZEXoP7CYvRV2ZwQ | William   |  126 |
| -9da1xk7zgnnfO1uTVYGkA | Fran      |  124 |
| -lh59ko3dxChBSZ9U7LfUw | Lissa     |  120 |
+------------------------+-----------+------+
		

Part 2: Inferences and Analysis

1. Pick one city and category of your choice and group the businesses in that city or category by their overall star rating. Compare the businesses with 2-3 stars to the businesses with 4-5 stars and answer the following questions. Include your code.

City: Las Vegas			Category: Restaurants
	
i. Do the two groups you chose to analyze have a different distribution of hours? Yes


ii. Do the two groups you chose to analyze have a different number of reviews? Yes
         
         
iii. Are you able to infer anything from the location data provided between these two groups? Explain.
The restaurants are in different area. Restaurants in 4-5 stars group are near by.

SQL code used for analysis:
select 
    (case
    when stars >=4 then '4-5 stars'
    when stars between 2 and 3 then '2-3 stars'
    else '1 stars' end) rating,
    b.name,
    b.review_count,
    b.neighborhood,
    b.address,
    b.postal_code,
    h.hours
from business b 
    join category c on b.id = c.business_id
    join hours h on b.id = h.business_id
where b.city = 'Las Vegas' and c.category = 'Restaurants' and b.stars >=2
order by b.stars desc

+-----------+---------------------+--------------+--------------+---------------------------------+-------------+-----------------------+
| rating    | name                | review_count | neighborhood | address                         | postal_code | hours                 |
+-----------+---------------------+--------------+--------------+---------------------------------+-------------+-----------------------+
| 4-5 stars | Big Wong Restaurant |          768 | Chinatown    | 5040 Spring Mountain Rd         | 89146       | Monday|10:00-23:00    |
| 4-5 stars | Big Wong Restaurant |          768 | Chinatown    | 5040 Spring Mountain Rd         | 89146       | Tuesday|10:00-23:00   |
| 4-5 stars | Big Wong Restaurant |          768 | Chinatown    | 5040 Spring Mountain Rd         | 89146       | Friday|10:00-23:00    |
| 4-5 stars | Big Wong Restaurant |          768 | Chinatown    | 5040 Spring Mountain Rd         | 89146       | Wednesday|10:00-23:00 |
| 4-5 stars | Big Wong Restaurant |          768 | Chinatown    | 5040 Spring Mountain Rd         | 89146       | Thursday|10:00-23:00  |
| 4-5 stars | Big Wong Restaurant |          768 | Chinatown    | 5040 Spring Mountain Rd         | 89146       | Sunday|10:00-23:00    |
| 4-5 stars | Big Wong Restaurant |          768 | Chinatown    | 5040 Spring Mountain Rd         | 89146       | Saturday|10:00-23:00  |
| 4-5 stars | Jacques Cafe        |          168 | Summerlin    | 1910 Village Center Cir, Unit 1 | 89134       | Monday|11:00-20:00    |
| 4-5 stars | Jacques Cafe        |          168 | Summerlin    | 1910 Village Center Cir, Unit 1 | 89134       | Tuesday|11:00-20:00   |
| 4-5 stars | Jacques Cafe        |          168 | Summerlin    | 1910 Village Center Cir, Unit 1 | 89134       | Friday|11:00-20:00    |
| 4-5 stars | Jacques Cafe        |          168 | Summerlin    | 1910 Village Center Cir, Unit 1 | 89134       | Wednesday|11:00-20:00 |
| 4-5 stars | Jacques Cafe        |          168 | Summerlin    | 1910 Village Center Cir, Unit 1 | 89134       | Thursday|11:00-20:00  |
| 4-5 stars | Jacques Cafe        |          168 | Summerlin    | 1910 Village Center Cir, Unit 1 | 89134       | Sunday|8:00-14:00     |
| 4-5 stars | Jacques Cafe        |          168 | Summerlin    | 1910 Village Center Cir, Unit 1 | 89134       | Saturday|11:00-20:00  |
| 2-3 stars | Wingstop            |          123 |              | 5045 W Tropicana Ave            | 89103       | Monday|11:00-0:00     |
| 2-3 stars | Wingstop            |          123 |              | 5045 W Tropicana Ave            | 89103       | Tuesday|11:00-0:00    |
| 2-3 stars | Wingstop            |          123 |              | 5045 W Tropicana Ave            | 89103       | Friday|11:00-0:00     |
| 2-3 stars | Wingstop            |          123 |              | 5045 W Tropicana Ave            | 89103       | Wednesday|11:00-0:00  |
| 2-3 stars | Wingstop            |          123 |              | 5045 W Tropicana Ave            | 89103       | Thursday|11:00-0:00   |
| 2-3 stars | Wingstop            |          123 |              | 5045 W Tropicana Ave            | 89103       | Sunday|11:00-0:00     |
| 2-3 stars | Wingstop            |          123 |              | 5045 W Tropicana Ave            | 89103       | Saturday|11:00-0:00   |
+-----------+---------------------+--------------+--------------+---------------------------------+-------------+-----------------------+	
		
2. Group business based on the ones that are open and the ones that are closed. What differences can you find between the ones that are still open and the ones that are closed? List at least two differences and the SQL code you used to arrive at your answer.
		
i. Difference 1: Average star of open restaurants is higher than the closed ones.
         
         
ii. Difference 2: Average reviews of open restaurants are higher than the closed ones.
         
         
         
SQL code used for analysis:
select 
    is_open,
    avg(stars) avg_stars,
    avg(review_count) avg_review
from business
group by is_open

+---------+-------------------+--------------------+
| is_open |         avg_stars |         avg_review |
+---------+-------------------+--------------------+
|       0 | 3.520394736842105 | 23.198026315789473 |
|       1 | 3.679009433962264 | 31.757075471698112 |
+---------+-------------------+--------------------+
	
	
3. For this last part of your analysis, you are going to choose the type of analysis you want to conduct on the Yelp dataset and are going to prepare the data for analysis.

Ideas for analysis include: Parsing out keywords and business attributes for sentiment analysis, clustering businesses to find commonalities or anomalies between them, predicting the overall star rating for a business, predicting the number of fans a user will have, and so on. These are just a few examples to get you started, so feel free to be creative and come up with your own problem you want to solve. Provide answers, in-line, to all of the following:
	
i. Indicate the type of analysis you chose to do:
Predicting number of fans a user will do would be an interesting analysis.        
         
ii. Write 1-2 brief paragraphs on the type of data you will need for your analysis and why you chose that data:
In order to prepare for the data, we identify tables and attributes needed for the analysis.
- user table
- friend table
- elite_years table
The first code is to perform descriptive analysis on the Yelp users - how many years they are members, number of posts, average ratio fans per review, useful and so on
The second code is to prepare the data for our objective prediction. The data is labeled data with the response variable (fans) and predictor variables ( years, review_count, avg_join_years, avg_fans_per_review, ...)                         
                  
iii. Output of your finished dataset:
Descriptive analysis:
- In average, Yelp users are members for 12 years
- Average fan per review is 0.07 meaning for every 12 posts expecting to gain 1 fan.
_ Average fan per useful is 0.443 meaning for every 2 posts with useful flag expecting to gain 1 fan.
_ Average fan per compliment is 0.163 meaning for every 6 posts with compliment flag expecting to gain 1 fan.     
         
iv. Provide the SQL code you used to create your final dataset:
Codes:

select 
    avg(Date('NOW') - u.yelping_since) avg_join_years,
    avg(fans*1.0/review_count) avg_fans_per_review,
    avg(fans*1.0/useful) avg_fans_per_useful,
    avg(fans*1.0/funny) avg_fans_per_funny,
    avg(fans*1.0/cool) avg_fans_per_cool,
    avg(fans*1.0/(compliment_hot+compliment_more+compliment_profile+compliment_cute+
            +compliment_list+compliment_note+compliment_plain+compliment_cool+compliment_cool
            +compliment_funny+compliment_writer+compliment_photos)) avg_fans_per_compliment
from user u
join elite_years e on u.id = e.user_id
left join 
    (select user_id, count(friend_id) friends from friend group by user_id) f 
    on u.id = f.user_id

Results:
+--------------------+---------------------+---------------------+--------------------+--------------------+-------------------------+
|     avg_join_years | avg_fans_per_review | avg_fans_per_useful | avg_fans_per_funny |  avg_fans_per_cool | avg_fans_per_compliment |
+--------------------+---------------------+---------------------+--------------------+--------------------+-------------------------+
| 12.741176470588234 | 0.07716272851505258 | 0.44355136489801955 | 0.8050196325066347 | 1.7174090616150617 |      0.1626870234707125 |
+--------------------+---------------------+---------------------+--------------------+--------------------+-------------------------+

Codes:
select
    u.name,
    u.review_count,
    u.fans,
    avg(Date('NOW') - u.yelping_since) avg_join_years,
    round(avg(fans*1.0/review_count),3) avg_fans_per_review,
    round(avg(fans*1.0/useful),3) avg_fans_per_useful,
    round(avg(fans*1.0/funny),3) avg_fans_per_funny,
    round(avg(fans*1.0/cool),3) avg_fans_per_cool,
    round(avg(fans*1.0/(compliment_hot+compliment_more+compliment_profile+compliment_cute+
            +compliment_list+compliment_note+compliment_plain+compliment_cool+compliment_cool
            +compliment_funny+compliment_writer+compliment_photos)),3) avg_fans_per_compliment
from user u
join elite_years e on u.id = e.user_id
group by u.id

Results:
+---------+--------------+------+----------------+---------------------+---------------------+--------------------+-------------------+-------------------------+
| name    | review_count | fans | avg_join_years | avg_fans_per_review | avg_fans_per_useful | avg_fans_per_funny | avg_fans_per_cool | avg_fans_per_compliment |
+---------+--------------+------+----------------+---------------------+---------------------+--------------------+-------------------+-------------------------+
| Sapna   |           38 |    1 |            7.0 |               0.026 |                None |               None |              None |                   0.333 |
| Dixie   |          503 |   41 |           13.0 |               0.082 |               1.952 |              1.281 |             1.783 |                   0.097 |
| Tasha   |          250 |    8 |           12.0 |               0.032 |               0.075 |              0.727 |             0.667 |                   0.242 |
| Lalena  |          224 |   25 |           10.0 |               0.112 |               0.056 |              0.248 |             0.115 |                   0.174 |
| Justin  |          177 |   13 |           12.0 |               0.073 |               0.283 |              1.182 |             1.444 |                   0.153 |
| Keith   |           61 |    3 |           10.0 |               0.049 |               0.176 |                1.5 |              None |                   0.333 |
| Brad    |          182 |    1 |           14.0 |               0.005 |               0.005 |              0.045 |               0.5 |                   0.048 |
| Nieves  |          178 |   80 |           11.0 |               0.449 |               0.073 |              0.103 |             0.085 |                   0.077 |
| Chris   |           70 |    2 |           14.0 |               0.029 |               0.035 |              0.286 |             0.182 |                   0.032 |
| Maung   |           54 |    0 |            9.0 |                 0.0 |                 0.0 |                0.0 |               0.0 |                     0.0 |
| Danial  |          136 |    5 |           18.0 |               0.037 |               0.417 |              1.667 |               5.0 |                   0.152 |
| Elaine  |          332 |   18 |           14.0 |               0.054 |               0.947 |              0.857 |             1.385 |                   0.154 |
| Matt    |          476 |   14 |           18.0 |               0.029 |               0.333 |              0.933 |              None |                   0.128 |
| Jamie   |           95 |    4 |           14.0 |               0.042 |               0.267 |               None |              None |                    0.16 |
| Jia     |          228 |    8 |           12.0 |               0.035 |               0.333 |              0.286 |               8.0 |                   0.333 |
| Mel     |          156 |    9 |           13.0 |               0.058 |               0.098 |              0.281 |             0.281 |                   0.107 |
| Ed      |          904 |   38 |           15.0 |               0.042 |                0.27 |              0.432 |             0.447 |                   0.286 |
| Tracy   |           71 |    5 |           10.0 |                0.07 |               0.833 |               1.25 |               5.0 |                   0.294 |
| Kristen |          428 |   15 |            9.0 |               0.035 |               0.333 |              1.364 |             2.143 |                   0.333 |
| Dominic |          836 |   37 |           13.0 |               0.044 |               0.457 |              1.423 |             0.712 |                   0.157 |
| Lissa   |          834 |  120 |           17.0 |               0.144 |               0.264 |                0.8 |             0.351 |                   0.043 |
+---------+--------------+------+----------------+---------------------+---------------------+--------------------+-------------------+-------------------------+